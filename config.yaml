dataset:
  train_path: /home/mvpavlukhin/speech2face/train.txt
  val_path: /home/mvpavlukhin/speech2face/test.txt
  image_size: 128

glow:
  image_shape: 
        - 128       
        - 128
        - 3

  #augment: True
  hidden_channels: 512
  K: 48
  L: 6
  actnorm_scale: 1.0
  flow_permutation: invconv # ["invconv", "shuffle", "reverse"]
  flow_coupling: additive  # choices=["additive", "affine"]
  LU_decomposed: True
  learn_top: True
  y_condition: False
  y_classes: 
  glow_weights: /home/mvpavlukhin/speech2face/output/glow_torch.ckpt

train_params:
  y_weight: 0.01 # "Weight for class condition loss"
  max_grad_norm: 5 # "Max norm of gradient (clip above - 0 for off)"
  n_workers: 4 # "number of data loading workers"
  batch_size: 12 # "batch size used during training"
  eval_batch_size: 12 # "batch size used during evaluation"
  epochs: 1000 # "number of epochs to train for"
  lr: 1e-3
  precision: 16
  accumulate_grad_batches: 5
 
env:
  output_dir: "./output/"
  fresh: True # "Remove output directory before starting"
  saved_model: "checkpoints"  # "Path to model to load for continuing training"
  saved_checkpoint: 
  seed: 666
  wandb_key: "21c78eafd4fee542c98beb82cab2ea36505fdc41"
  db: 
  num_nodes: 4
  num_gpu: 1
