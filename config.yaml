dataset: celeba
dataroot: './'
download: True
augment: True
hidden_channels: 512
K: 48
L: 4
actnorm_scale: 1.0
flow_permutation: invconv # ["invconv", "shuffle", "reverse"]
flow_coupling: affine  # choices=["additive", "affine"]
LU_decomposed: False
learn_top: False
y_condition: False
y_weight: 0.01 # "Weight for class condition loss"
max_grad_clip: 0.5 # "Max gradient value (clip above - for off)"
max_grad_norm: 0.5 # "Max norm of gradient (clip above - 0 for off)"
n_workers: 4 # "number of data loading workers"
batch_size: 32 # "batch size used during training"
eval_batch_size: 64 # "batch size used during evaluation"
epochs: 1000 # "number of epochs to train for"
opt_type: 'AdamW' #name of optimizer
lr: 5e-5
use_swa: False #using of Stochastic Weight Averaging
swa_start: 0 # when start swa, 0 if disabled
swa_lr: 0 # learning rate of swa, 0 if disabled
warmup: 5 # "Use this number of epochs to warmup learning rate linearly from zero to learning rate"
n_init_batches: 8  # "Number of batches to use for Act Norm initialisation"
cuda: True
output_dir: "./output/"
fresh: True # "Remove output directory before starting"
saved_model: "./models" # "Path to model to load for continuing training"
saved_optimizer: "./optimizers" #"Path to optimizer to load for continuing training"
seed: 666
precision: 16
num_gpu: 1
accumulate_grad_batches: 4